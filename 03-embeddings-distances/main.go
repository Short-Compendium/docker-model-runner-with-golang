package main

import (
	"context"
	"embeddings-demo/rag"
	"fmt"
	"log"
	"os"

	"github.com/openai/openai-go"
	"github.com/openai/openai-go/option"
)

var chunks = []string{
	`The lions run in the savannah`,
	`The birds fly in the sky`,
	`The frogs swim in the pond`,
	`The fish swim in the sea`,
}

// MODEL_RUNNER_BASE_URL=http://localhost:12434 go run main.go
func main() {
	ctx := context.Background()

	llmURL := os.Getenv("MODEL_RUNNER_BASE_URL") + "/engines/llama.cpp/v1/"
	embeddingsModel := "ai/mxbai-embed-large"

	client := openai.NewClient(
		option.WithBaseURL(llmURL),
		option.WithAPIKey(""),
	)

	// -------------------------------------------------
	// Create a vector store
	// -------------------------------------------------
	store := rag.MemoryVectorStore{
		Records: make(map[string]rag.VectorRecord),
	}

	// -------------------------------------------------
	// Create and save the embeddings from the chunks
	// -------------------------------------------------
	fmt.Println("‚è≥ Creating embeddings from chunks...")

	for _, chunk := range chunks {
		embeddingsResponse, err := client.Embeddings.New(ctx, openai.EmbeddingNewParams{
			Input: openai.EmbeddingNewParamsInputUnion{
				OfString: openai.String(chunk),
			},
			Model: embeddingsModel,
		})

		if err != nil {
			fmt.Println(err)
		} else {
			_, errSave := store.Save(rag.VectorRecord{
				Prompt:    chunk,
				Embedding: embeddingsResponse.Data[0].Embedding,
			})

			if errSave != nil {
				fmt.Println("üò°:", errSave)
			}
		}
	}

	fmt.Println("‚úã", "Embeddings created, total of records", len(store.Records))
	fmt.Println()

	// -------------------------------------------------
	// Search for similarities
	// -------------------------------------------------
	//userQuestion := "What does the lion do?"
	//userQuestion := "Where are the birds?"
	//userQuestion := "What can be found in the pond?"
	userQuestion := "Which animals swim?"

	fmt.Println("‚è≥ Searching for similarities...")

	// -------------------------------------------------
	// Create embedding from the user question
	// -------------------------------------------------
	embeddingsResponse, err := client.Embeddings.New(ctx, openai.EmbeddingNewParams{
		Input: openai.EmbeddingNewParamsInputUnion{
			OfString: openai.String(userQuestion),
		},
		Model: embeddingsModel,
	})
	if err != nil {
		log.Fatal("üò°:", err)
	}
	// -------------------------------------------------
	// Create a vector record from the user embedding
	// -------------------------------------------------
	embeddingFromUserQuestion := rag.VectorRecord{
		//Prompt:    userQuestion,
		Embedding: embeddingsResponse.Data[0].Embedding,
	}

	//similarities, _  := store.SearchSimilarities(embeddingFromUserQuestion, 0.6)

	similarities, _ := store.SearchTopNSimilarities(embeddingFromUserQuestion, 0.6, 2)
	// if the limit is to near from 1, the risk is to lose the best match

	//documentsContent := "Documents:\n"

	for _, similarity := range similarities {
		fmt.Println("‚úÖ CosineSimilarity:", similarity.CosineSimilarity, "Chunk:", similarity.Prompt)
		//documentsContent += fmt.Sprintf("<doc>%s</doc>\n", similarity.Prompt)
		//documentsContent += similarity.Prompt
	}
	//documentsContent += "\n"
	fmt.Println("‚úã", "Similarities found, total of records", len(similarities))
	fmt.Println()

}
